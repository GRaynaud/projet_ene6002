\section{Présentation de la méthode PINN}

\subsection{Introduction et état de l'art}

L'utilisation des techniques d'apprentissage machine aux domaines dits "classiques" comme la mécanique n'est pas nouveau. De nombreux travaux de recherche ont été menés jusqu'aux applications industrielles pour des problèmes d'aide à l'optimisation, de contrôle, de pré-développement de pièces ou de modèles réduits dont on pourra se faire une idée avec la synthèse de \cite{bruntonMachineLearningFluid2019a}. Néanmoins l'approche privilégiée a été celle des données, \textit{data-driven} comme on le trouve dans la littérature. Cela signifie que les réseaux de neurones encodent les mécanismes physiques (dans le meilleur des cas) uniquement à partir d'un certain nombre de données préparées au préalable. Par exemple,  \cite{guoConvolutionalNeuralNetworks2016} montre qu'il est possible d'estimer très rapidement l'écoulement moyen autour de formes complexes à l'aide de réseaux de neurones convolutionnels (CNNs) (Pour les concepts de base on pourra se référer à un ouvrage de référence \cite{goodfellowDeepLearning2016a}). \\

Néanmoins une autre approche a été récemment explorée, celle de l'approximation des solutions d'un problème de type PDE (équations aux dérivées partielles) par des familles de fonctions sous l'hypothèse du théorème général d'approximation. S. Brunton a notamment travaillé dans ce domaine en déterminant les fonctions les plus adaptées pour décrire quantitativement avec des données brutes les solutions d'un problème à partir d'une famille suffisamment large de fonctions candidates \cite{bruntonDiscoveringGoverningEquations2016}. Une suite logique est présentée chez \cite{corbettaApplicationSparseIdentification} où la représentation d'un phénomène par une famille de fonctions continues permet de remonter aux équations qui le gouvernent. C'est aussi une manière d'établir des modèles réduits de problèmes complexes. \\

Le formalisme plus classique des réseaux de neurones comme détaillé dans la section suivante apparaît un peu plus tard. \cite{raissiPhysicsinformedNeuralNetworks2019} présente un formalisme assez général de résolution de PDE avec différentes échelles de complexité et non-linéarités que l'on appelera PINN par la suite (pour \textit{Physics-Informed Neural Networks}. Il s'agit de représenter les solutions d'une PDE par une combinaison de fonctions de bases, connues, dont les inputs sont les coordonnées physiques : le temps, l'espace. Il n'y a donc pas de discrétisation spatiale ou temporelle. De plus, il introduit des fonctions d'erreurs basées sur les équations locales du problème accompagnées de conditions aux limites (spatiales et temporelles) ainsi que d'écarts aux mesures. On pourra également trouver une justification mathématique de la méthode avec une analogie avec la méthode de Galerkin chez \cite{al-aradiSolvingNonlinearHighDimensional}. Le manuscrit de thèse \cite{rudyComputationalMethodsSystem2019} est également une piste d'approfondissement.\\

Des applications dans des domaines plus spécifiques sont rapidement publiées : \cite{raissiHiddenFluidMechanics2018} pour la reconstitution d'écoulement et de forces à partir de mesures de concentration d'un scalaire passif. Dans le domaine de l'interaction fluide-structure, \cite{raissiDeepLearningVortexinduced2019a} propose la reconstitution d'écoulement 2D autour d'un cylindre monté sur ressort à partir de divers types de données initiales. Plus récemment encore, on pourra se pencher du côté de \cite{maoPhysicsinformedNeuralNetworks2020} pour des écoulements hautes-vitesses, \cite{haghighatDeepLearningFramework2020,luExtractionMechanicalProperties2020} pour de la mécanique des solides ou encore \cite{chenPhysicsinformedNeuralNetworks2020} dans le domaine de la nano-optique et des méta matériaux.\


\subsection{Principe de fonctionnement}

L'idée fondamentale de la méthode PINN réside dans le théorème général d'approximation \cite{hornikMultilayerFeedforwardNetworks1989} que l'on pourra énoncer dans une version simplifiée par : 

\begin{theorem}
Soit $u : x\in  C \rightarrow u(x) \in \mathbb{R}^m$ une fonction continue d'un sous-ensemble compact $C$ de $\mathbb{R}^n$ dans $\mathbb{R}^m$. Alors $u$ peut-être approchée avec une fonction d'activation $f$ avec une précision (étant donné une norme sur $C$) arbitraire (i.e. à $\epsilon \in \mathbb{R}^+$ près) par un réseau à une couche cachée suffisamment large ($p$).
$$ \forall \epsilon > 0, \exists p\in \mathbb{N}, B_1 \in \mathbb{R}^p, B_2 \in \mathbb{R}^m, W_1 \in \mathbb{R}^{p\times n}, W_2 \in \mathbb{R}^{m \times p} $$
définissant
$$ \Tilde{u}_{NN} (x) = W_2 f\left( W_1 . x + B_1  \right) + B_2 $$
Avec 
$$ \| \Tilde{u}_{DNN} - u \|_{C} \leq \epsilon $$
\end{theorem}

L'approximation de $u$ par un réseau de neurones mono-couche $\Tilde{u}_{DNN}$ est construite en plusieurs étapes successives : 
\begin{enumerate}
    \item D'abord, on effectue une première opération matricielle de l'ensemble de départ ($x\in \mathbb{R}^n$) jusqu'à $\mathbb{R}^p$ qui est l'espace de la couche cachée (\textit{hidden layer} dans la littérature. On multiplie $x$ par la matrice $W_1$ puis on ajoute un terme constant $B_1$. 
    $$ y_1 = W_1 . x + B_1 $$
    \item On effectue une opération non-linéaire sur les éléments du hidden-layer avec la fonction d'activation $f$. Par exemple on peut prendre la tangente hyperbolique de chacun des éléments du vecteur $y_1 \in \mathbb{R}^p$.
    \item On envoie $f(y_1)$ dans l'espace d'arrivée $\mathbb{R}^m$ avec une seconde opération matricielle (comme au 1.) utilisant $W_2$ et $B_2$ :
    $$ \Tilde{u}_{DNN} = W_2 f(y_1) + B_2 $$
\end{enumerate}
En pratique le choix de la taille de la couche cachée $p$ ainsi que les fonctions d'activations $f$ ne sont pas donnés et peuvent dépendre du problème en question. On trouve beaucoup de littérature à ce sujet, par exemple sur les fonctions d'activation \cite{leshnoMultilayerFeedforwardNetworks1993}.\\

D'autre part, les matrices $(W_k)_k$ et les vecteurs $(B_k)_k$ sont appelés poids (W pour \textit{Weights} en anglais) et biais et forment l'ensemble des paramètres ajustables du modèle. En pratique, pour éviter que $p$ ne soit trop grand quand $u$ se complexifie, on rajoute des couches intérieures, "cachées". Cela signifie qu'après l'étape 2, on reprend l'étape 1 avec de nouveaux poids $W$ et biais $B$. On peut recommencer autant de fois que l'on veut de couches cachées (on parle de profondeur du réseau). On effectue l'étape 3 pour revenir dans l'espace d'arrivée de $u(x)$. \\

Intuitivement ce résultat peut se comprendre avec l'exemple plus classique du Théorème de Stone-Weierstrass (approximation des fonctions continues par un polynôme et décomposition en série de polynômes de Taylor). Pour approcher une fonction continue, on ne travaille pas dans un espace de fonctions de bases constantes (ou $\mathcal{C}^k$) par morceaux sur une partition d'un intervalle  $I \subset R$, ce qui est le fondement des discrétisations spatiales par un maillage. Au lieu de ça, on travaille dans un espace de paramètres de fonctions continues. Par exemple dans le cas de Stone-Weierstrass, ces paramètres sont les coefficients associés à une base de polynômes. Pour un réseau de neurones, l'espace des paramètres est celui des poids et des biais. On désigne généralement ces paramètres par $\theta$.\\

Finalement, connaissant la structure du réseau, c'est à dire ses fonctions d'activation $f$, le nombre de couches cachées ainsi que la taille de chacune de ses couches cachées, $\Tilde{u}_{DNN}$ est entièrement définie avec l'ensemble des paramètres $\theta$ (poids et biais) pour tout $x\in \mathbb{R}^n$. L'objectif du problème est donc d'\textbf{optimiser} ces paramètres en \textbf{minimisant une erreur} $\mathcal {L}$. La difficulté étant de bien construire cette erreur.\\

L'optimisation d'un très grand nombre de paramètres ($\theta$ peut contenir plusieurs milliers de scalaires !) est rendue possible par des outils de calcul symbolique. En effet, pour chacun des $\theta_i$, on peut calculer \textbf{symboliquement} $\pder[\mathcal{L}]{\theta_i}$ par dérivation en chaîne : $\pder[f(g)]{\theta_i} = \pder[f]{g} \pder[g]{\theta_i}$ puisque on connaît explicitement l'ensemble des opérations qui permettent d'obtenir $\Tilde{u}_{DNN}$ puis $\mathcal{L}$ à partir de $x$ et $\theta$. Et plus important, on sait les dériver exactement ! Enfin on a des librairies qui le font automatiquement (Tensorflow par exemple \cite{PartialDifferentialEquations}) : on appelle cela l'\textbf{auto-différenciation}. \\

L'auto-différenciation permet donc de dériver symboliquement l'erreur $\mathcal{L}$ par rapport aux paramètres du modèle et ainsi faire des descentes de gradients ou de Hessiennes. Plusieurs optimiseurs existent et font cela en boite noire. En particulier, on 



\subsection{Adaptation au problème du projet et limites}

